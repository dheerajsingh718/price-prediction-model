{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "improved-aggregation",
   "metadata": {},
   "source": [
    "# Improved Data Aggregation Pipeline\n",
    "\n",
    "## Key Improvements:\n",
    "1. âœ… Robust date parsing and validation\n",
    "2. âœ… Automatic data type conversion\n",
    "3. âœ… Comprehensive logging and error handling\n",
    "4. âœ… Data quality checks and validation\n",
    "5. âœ… Intelligent column mapping\n",
    "6. âœ… Schema validation across files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configuration",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "FOLDER_PATH = \"../datasets\"\n",
    "OUTPUT_CSV = \"../datasets/combined_clean.csv\"\n",
    "OUTPUT_EXCEL = \"../merged_data.xlsx\"\n",
    "LOG_FILE = \"data_aggregation_log.txt\"\n",
    "\n",
    "# Columns to drop (non-essential)\n",
    "COLUMNS_TO_DROP = [\n",
    "    'Balance', 'Remaning', 'Achived', 'Bills',\n",
    "    'G- Shock', 'G-sale', 'G-profit', 'TChain', 'Tbracelet',\n",
    "    'Zippo', 'Store Time', 'Unnamed: 15', 'Unnamed: 4', 'Unnamed: 6',\n",
    "    'Tips: Jay'\n",
    "]\n",
    "\n",
    "# Expected date column names (case-insensitive)\n",
    "DATE_COLUMNS = ['date', 'Date', 'DATE']\n",
    "\n",
    "# Employee name mappings (handle variations)\n",
    "EMPLOYEE_MAPPINGS = {\n",
    "    'dheeraj.1': 'dheeraj_hours',\n",
    "    'shashi.1': 'shashi_hours',\n",
    "    'yash.1': 'yash_hours',\n",
    "    'nithin.1': 'nithin_hours',\n",
    "    'sunny.1': 'sunny_hours',\n",
    "    'siddharth.1': 'siddharth_hours',\n",
    "    'atharv.1': 'atharv_hours'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helper-functions",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "helpers",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logging(log_file=LOG_FILE):\n",
    "    \"\"\"Setup logging to file and console\"\"\"\n",
    "    import logging\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_file, mode='w'),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    return logging.getLogger(__name__)\n",
    "\n",
    "logger = setup_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "parse-date",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_date_flexible(date_value):\n",
    "    \"\"\"Parse date with multiple format attempts\"\"\"\n",
    "    if pd.isna(date_value):\n",
    "        return None\n",
    "    \n",
    "    # If already datetime, return as is\n",
    "    if isinstance(date_value, pd.Timestamp):\n",
    "        return date_value\n",
    "    \n",
    "    # Try common date formats\n",
    "    date_formats = [\n",
    "        '%Y-%m-%d',\n",
    "        '%m/%d/%Y',\n",
    "        '%d/%m/%Y',\n",
    "        '%Y-%m-%d %H:%M:%S',\n",
    "        '%m-%d-%Y',\n",
    "        '%d-%m-%Y'\n",
    "    ]\n",
    "    \n",
    "    for fmt in date_formats:\n",
    "        try:\n",
    "            return pd.to_datetime(date_value, format=fmt)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # Last resort: pandas auto-detect\n",
    "    try:\n",
    "        return pd.to_datetime(date_value, errors='coerce')\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "extract-month-year",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_month_year_from_filename(filename):\n",
    "    \"\"\"Extract month and year from filename\"\"\"\n",
    "    # Remove extension\n",
    "    name = Path(filename).stem\n",
    "    \n",
    "    # Month name to number mapping\n",
    "    months = {\n",
    "        'january': 1, 'jan': 1,\n",
    "        'february': 2, 'feb': 2,\n",
    "        'march': 3, 'mar': 3,\n",
    "        'april': 4, 'apr': 4,\n",
    "        'may': 5,\n",
    "        'june': 6, 'jun': 6,\n",
    "        'july': 7, 'jul': 7,\n",
    "        'august': 8, 'aug': 8,\n",
    "        'september': 9, 'sep': 9, 'september': 9,\n",
    "        'october': 10, 'oct': 10,\n",
    "        'november': 11, 'nov': 11,\n",
    "        'december': 12, 'dec': 12\n",
    "    }\n",
    "    \n",
    "    name_lower = name.lower()\n",
    "    \n",
    "    # Extract year (4 digits)\n",
    "    year_match = re.search(r'\\b(20\\d{2})\\b', name)\n",
    "    year = int(year_match.group(1)) if year_match else None\n",
    "    \n",
    "    # Extract month\n",
    "    month = None\n",
    "    for month_name, month_num in months.items():\n",
    "        if month_name in name_lower:\n",
    "            month = month_num\n",
    "            break\n",
    "    \n",
    "    return month, year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "normalize-columns",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_column_names(df):\n",
    "    \"\"\"Normalize column names: lowercase, strip, handle duplicates\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Step 1: Strip and lowercase\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "    \n",
    "    # Step 2: Identify duplicate column names\n",
    "    counts = Counter(df.columns)\n",
    "    duplicate_groups = {name: [] for name, count in counts.items() if count > 1}\n",
    "    \n",
    "    # Step 3: Group duplicate columns by name and get their positions\n",
    "    for idx, col_name in enumerate(df.columns):\n",
    "        if col_name in duplicate_groups:\n",
    "            duplicate_groups[col_name].append(idx)\n",
    "    \n",
    "    # Step 4: Combine duplicate columns and build new dataframe\n",
    "    combined_data = {}\n",
    "    processed_duplicates = set()\n",
    "    \n",
    "    for idx, col_name in enumerate(df.columns):\n",
    "        # Skip if this column is a duplicate we've already processed\n",
    "        if col_name in processed_duplicates:\n",
    "            continue\n",
    "            \n",
    "        if col_name in duplicate_groups:\n",
    "            # This is a duplicate group - combine them\n",
    "            duplicate_indices = duplicate_groups[col_name]\n",
    "            \n",
    "            # Get all duplicate columns\n",
    "            duplicate_cols = df.iloc[:, duplicate_indices]\n",
    "            \n",
    "            # Combine using backfill (fill missing values from right to left)\n",
    "            combined = duplicate_cols.bfill(axis=1).iloc[:, 0]\n",
    "            \n",
    "            combined_data[col_name] = combined\n",
    "            \n",
    "            # Mark all duplicates as processed\n",
    "            processed_duplicates.add(col_name)\n",
    "        else:\n",
    "            # Regular column - keep as is\n",
    "            combined_data[col_name] = df.iloc[:, idx]\n",
    "    \n",
    "    # Step 5: Create new dataframe with unique columns\n",
    "    result_df = pd.DataFrame(combined_data)\n",
    "    \n",
    "    # Step 6: Ensure no remaining duplicates (safety check)\n",
    "    result_df = result_df.loc[:, ~result_df.columns.duplicated()]\n",
    "    \n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "convert-data-types",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_types(df):\n",
    "    \"\"\"Convert data types intelligently\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Parse date column\n",
    "    date_col = None\n",
    "    for col in DATE_COLUMNS:\n",
    "        if col.lower() in df.columns:\n",
    "            date_col = col.lower()\n",
    "            break\n",
    "    \n",
    "    if date_col:\n",
    "        df[date_col] = df[date_col].apply(parse_date_flexible)\n",
    "    \n",
    "    # Convert numeric columns (employee sales, hours, tips)\n",
    "    numeric_patterns = [\n",
    "        'hours', 'tips', 'total', 'jay', 'yash', 'dheeraj',\n",
    "        'nithin', 'shashi', 'atharv', 'sunny', 'siddharth'\n",
    "    ]\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower()\n",
    "        \n",
    "        # Skip date and day columns\n",
    "        if col_lower in ['date', 'day', 'time', 'source_file', 'source file']:\n",
    "            continue\n",
    "        \n",
    "        # Check if column should be numeric\n",
    "        if any(pattern in col_lower for pattern in numeric_patterns):\n",
    "            # Try to convert to numeric\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "read-excel-file",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_excel_file(file_path, file_name):\n",
    "    \"\"\"Read Excel file with error handling and validation\"\"\"\n",
    "    try:\n",
    "        # Try reading without row limit first to check file size\n",
    "        df_temp = pd.read_excel(file_path, sheet_name=0, nrows=5)\n",
    "        \n",
    "        # Read full file (nrows=32 limit)\n",
    "        df = pd.read_excel(os.path.join(folder_path, file),nrows=32, sheet_name=0)\n",
    "        \n",
    "        # Add metadata\n",
    "        df['source_file'] = file_name\n",
    "        \n",
    "        # Extract month/year from filename\n",
    "        month, year = extract_month_year_from_filename(file_name)\n",
    "        if month:\n",
    "            df['file_month'] = month\n",
    "        if year:\n",
    "            df['file_year'] = year\n",
    "        \n",
    "        logger.info(f\"âœ… Successfully read {file_name}: {len(df)} rows, {len(df.columns)} columns\")\n",
    "        \n",
    "        return df, None\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"âŒ Failed to read {file_name}: {str(e)}\"\n",
    "        logger.error(error_msg)\n",
    "        return None, error_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "validate-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data_quality(df, file_name):\n",
    "    \"\"\"Validate data quality and return report\"\"\"\n",
    "    report = {\n",
    "        'file': file_name,\n",
    "        'rows': len(df),\n",
    "        'columns': len(df.columns),\n",
    "        'missing_dates': 0,\n",
    "        'missing_totals': 0,\n",
    "        'duplicate_rows': 0,\n",
    "        'date_range': None\n",
    "    }\n",
    "    \n",
    "    # Check for date column\n",
    "    date_col = None\n",
    "    for col in DATE_COLUMNS:\n",
    "        if col.lower() in df.columns:\n",
    "            date_col = col.lower()\n",
    "            break\n",
    "    \n",
    "    if date_col:\n",
    "        report['missing_dates'] = df[date_col].isna().sum()\n",
    "        valid_dates = df[date_col].dropna()\n",
    "        \n",
    "        if len(valid_dates) > 0:\n",
    "            # Convert to datetime if needed (handle mixed types)\n",
    "            try:\n",
    "                # Try to convert to datetime if not already\n",
    "                if not pd.api.types.is_datetime64_any_dtype(valid_dates):\n",
    "                    valid_dates = pd.to_datetime(valid_dates, errors='coerce')\n",
    "                \n",
    "                # Only compute min/max if we have valid datetime values\n",
    "                valid_datetime = valid_dates.dropna()\n",
    "                if len(valid_datetime) > 0:\n",
    "                    report['date_range'] = (valid_datetime.min(), valid_datetime.max())\n",
    "            except Exception as e:\n",
    "                # If conversion fails, skip date range\n",
    "                logger.warning(f\"Could not compute date range for {file_name}: {e}\")\n",
    "                report['date_range'] = None\n",
    "    \n",
    "    # Check for total column\n",
    "    if 'total' in df.columns:\n",
    "        report['missing_totals'] = df['total'].isna().sum()\n",
    "    \n",
    "    # Check for duplicates\n",
    "    if date_col:\n",
    "        report['duplicate_rows'] = df.duplicated(subset=[date_col]).sum()\n",
    "    \n",
    "    return report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "main-pipeline",
   "metadata": {},
   "source": [
    "## Main Aggregation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "get-files",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-28 09:21:29,712 - INFO - ðŸ“ Found 32 Excel files to process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ Found 32 Excel files to process\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get all Excel files\n",
    "folder_path = Path(FOLDER_PATH)\n",
    "all_files = sorted([f for f in os.listdir(folder_path) if f.endswith(\".xlsx\")])\n",
    "\n",
    "logger.info(f\"ðŸ“ Found {len(all_files)} Excel files to process\")\n",
    "print(f\"\\nðŸ“ Found {len(all_files)} Excel files to process\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "process-files",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-28 09:21:30,123 - INFO - âœ… Successfully read April 2025.xlsx: 32 rows, 24 columns\n",
      "2026-01-28 09:21:30,155 - INFO - âœ… Successfully read April24.xlsx: 30 rows, 25 columns\n",
      "2026-01-28 09:21:30,182 - INFO - âœ… Successfully read August 2023.xlsx: 31 rows, 23 columns\n",
      "2026-01-28 09:21:30,209 - INFO - âœ… Successfully read August 2025.xlsx: 31 rows, 24 columns\n",
      "2026-01-28 09:21:30,418 - INFO - âœ… Successfully read August 24.xlsx: 31 rows, 26 columns\n",
      "2026-01-28 09:21:30,444 - INFO - âœ… Successfully read December 2023.xlsx: 31 rows, 25 columns\n",
      "2026-01-28 09:21:30,567 - INFO - âœ… Successfully read December 2025.xlsx: 31 rows, 17 columns\n",
      "2026-01-28 09:21:30,597 - INFO - âœ… Successfully read December2022.xlsx: 31 rows, 24 columns\n",
      "2026-01-28 09:21:30,717 - INFO - âœ… Successfully read December2024.xlsx: 31 rows, 26 columns\n",
      "2026-01-28 09:21:30,741 - INFO - âœ… Successfully read Feb 2023.xlsx: 31 rows, 23 columns\n",
      "2026-01-28 09:21:30,769 - INFO - âœ… Successfully read Feb2024.xlsx: 32 rows, 24 columns\n",
      "2026-01-28 09:21:30,894 - INFO - âœ… Successfully read Feb2025.xlsx: 31 rows, 26 columns\n",
      "2026-01-28 09:21:30,979 - INFO - âœ… Successfully read Jan 2024.xlsx: 31 rows, 25 columns\n",
      "2026-01-28 09:21:31,098 - INFO - âœ… Successfully read Jan2025.xlsx: 32 rows, 26 columns\n",
      "2026-01-28 09:21:31,217 - INFO - âœ… Successfully read January 2026.xlsx: 31 rows, 17 columns\n",
      "2026-01-28 09:21:31,242 - INFO - âœ… Successfully read July 2023.xlsx: 31 rows, 23 columns\n",
      "2026-01-28 09:21:31,365 - INFO - âœ… Successfully read July 2025.xlsx: 31 rows, 24 columns\n",
      "2026-01-28 09:21:31,541 - INFO - âœ… Successfully read July 24.xlsx: 32 rows, 26 columns\n",
      "2026-01-28 09:21:31,666 - INFO - âœ… Successfully read June 2025.xlsx: 30 rows, 24 columns\n",
      "2026-01-28 09:21:31,788 - INFO - âœ… Successfully read June24.xlsx: 32 rows, 26 columns\n",
      "2026-01-28 09:21:31,818 - INFO - âœ… Successfully read March 2024.xlsx: 32 rows, 25 columns\n",
      "2026-01-28 09:21:32,032 - INFO - âœ… Successfully read March2025.xlsx: 32 rows, 21 columns\n",
      "2026-01-28 09:21:32,140 - INFO - âœ… Successfully read May 2025.xlsx: 31 rows, 24 columns\n",
      "2026-01-28 09:21:32,170 - INFO - âœ… Successfully read NOVEMBER 2022.xlsx: 31 rows, 25 columns\n",
      "2026-01-28 09:21:32,291 - INFO - âœ… Successfully read November 2025.xlsx: 30 rows, 24 columns\n",
      "2026-01-28 09:21:32,467 - INFO - âœ… Successfully read November 24.xlsx: 31 rows, 26 columns\n",
      "2026-01-28 09:21:32,496 - INFO - âœ… Successfully read November2023.xlsx: 31 rows, 24 columns\n",
      "2026-01-28 09:21:32,523 - INFO - âœ… Successfully read October 2023.xlsx: 31 rows, 25 columns\n",
      "2026-01-28 09:21:32,646 - INFO - âœ… Successfully read October 2025.xlsx: 31 rows, 24 columns\n",
      "2026-01-28 09:21:32,767 - INFO - âœ… Successfully read October 24.xlsx: 31 rows, 26 columns\n",
      "2026-01-28 09:21:32,884 - INFO - âœ… Successfully read September24.xlsx: 30 rows, 26 columns\n",
      "2026-01-28 09:21:33,065 - INFO - âœ… Successfully read Septermber 2025.xlsx: 31 rows, 24 columns\n",
      "2026-01-28 09:21:33,069 - INFO - \n",
      "âœ… Successfully processed 32 files\n",
      "2026-01-28 09:21:33,069 - INFO - âŒ Failed to process 0 files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Successfully processed 32 files\n",
      "âŒ Failed to process 0 files\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Process all files\n",
    "dfs = []\n",
    "errors = []\n",
    "quality_reports = []\n",
    "\n",
    "for file in all_files:\n",
    "    file_path = folder_path / file\n",
    "    \n",
    "    # Read file\n",
    "    df, error = read_excel_file(file_path, file)\n",
    "    \n",
    "    if df is None:\n",
    "        errors.append({'file': file, 'error': error})\n",
    "        continue\n",
    "    \n",
    "    # Normalize columns\n",
    "    df = normalize_column_names(df)\n",
    "    \n",
    "    # Validate data quality\n",
    "    quality_report = validate_data_quality(df, file)\n",
    "    quality_reports.append(quality_report)\n",
    "    \n",
    "    # Convert data types\n",
    "    df = convert_data_types(df)\n",
    "    \n",
    "    # Apply employee column mappings\n",
    "    df = df.rename(columns=EMPLOYEE_MAPPINGS)\n",
    "    \n",
    "    dfs.append(df)\n",
    "\n",
    "logger.info(f\"\\nâœ… Successfully processed {len(dfs)} files\")\n",
    "logger.info(f\"âŒ Failed to process {len(errors)} files\")\n",
    "\n",
    "print(f\"\\nâœ… Successfully processed {len(dfs)} files\")\n",
    "print(f\"âŒ Failed to process {len(errors)} files\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "combine-dataframes",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-28 09:21:33,093 - INFO - ðŸ“Š Combined dataset: 995 rows, 41 columns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Combined dataset: 995 rows, 41 columns\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combine all dataframes\n",
    "if dfs:\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    logger.info(f\"ðŸ“Š Combined dataset: {len(combined_df)} rows, {len(combined_df.columns)} columns\")\n",
    "    print(f\"\\nðŸ“Š Combined dataset: {len(combined_df)} rows, {len(combined_df.columns)} columns\\n\")\n",
    "else:\n",
    "    logger.error(\"No dataframes to combine!\")\n",
    "    raise ValueError(\"No dataframes to combine!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "clean-columns",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-28 09:21:33,101 - INFO - ðŸ—‘ï¸  Dropped 15 unnecessary columns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ—‘ï¸  Dropped 15 unnecessary columns\n"
     ]
    }
   ],
   "source": [
    "# Drop unnecessary columns (case-insensitive)\n",
    "columns_to_drop_lower = [col.lower() for col in COLUMNS_TO_DROP]\n",
    "existing_cols_to_drop = [col for col in combined_df.columns if col.lower() in columns_to_drop_lower]\n",
    "\n",
    "if existing_cols_to_drop:\n",
    "    combined_df = combined_df.drop(columns=existing_cols_to_drop)\n",
    "    logger.info(f\"ðŸ—‘ï¸  Dropped {len(existing_cols_to_drop)} unnecessary columns\")\n",
    "    print(f\"ðŸ—‘ï¸  Dropped {len(existing_cols_to_drop)} unnecessary columns\")\n",
    "\n",
    "# Final column normalization\n",
    "combined_df = normalize_column_names(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "data-summary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATA SUMMARY\n",
      "============================================================\n",
      "Total Rows: 995\n",
      "Total Columns: 26\n",
      "\n",
      "Column Names:\n",
      "   1. date\n",
      "   2. day\n",
      "   3. time\n",
      "   4. jay\n",
      "   5. dheeraj\n",
      "   6. shashi\n",
      "   7. total\n",
      "   8. dheeraj_hours\n",
      "   9. shashi_hours\n",
      "  10. tips: shashi\n",
      "  11. tips: dheeraj\n",
      "  12. source_file\n",
      "  13. file_month\n",
      "  14. file_year\n",
      "  15. yash\n",
      "  16. nithin\n",
      "  17. yash_hours\n",
      "  18. total_employees_hours\n",
      "  19. total_employees\n",
      "  20. nithin_hours\n",
      "  21. tips: yash\n",
      "  22. sunny\n",
      "  23. sunny_hours\n",
      "  24. siddharth\n",
      "  25. atharv\n",
      "  26. atharv_hours\n",
      "\n",
      "============================================================\n",
      "DATA TYPES\n",
      "============================================================\n",
      "date                     datetime64[ns]\n",
      "day                              object\n",
      "time                             object\n",
      "jay                             float64\n",
      "dheeraj                         float64\n",
      "shashi                          float64\n",
      "total                           float64\n",
      "dheeraj_hours                   float64\n",
      "shashi_hours                    float64\n",
      "tips: shashi                    float64\n",
      "tips: dheeraj                   float64\n",
      "source_file                      object\n",
      "file_month                        int64\n",
      "file_year                       float64\n",
      "yash                            float64\n",
      "nithin                          float64\n",
      "yash_hours                      float64\n",
      "total_employees_hours           float64\n",
      "total_employees                 float64\n",
      "nithin_hours                    float64\n",
      "tips: yash                      float64\n",
      "sunny                           float64\n",
      "sunny_hours                     float64\n",
      "siddharth                       float64\n",
      "atharv                          float64\n",
      "atharv_hours                    float64\n",
      "dtype: object\n",
      "\n",
      "============================================================\n",
      "MISSING VALUES\n",
      "============================================================\n",
      "               Column  Missing Count  Missing %\n",
      "            siddharth            995     100.00\n",
      "               atharv            994      99.90\n",
      "         atharv_hours            992      99.70\n",
      "                sunny            992      99.70\n",
      "          sunny_hours            972      97.69\n",
      "      total_employees            965      96.98\n",
      "total_employees_hours            965      96.98\n",
      "           tips: yash            954      95.88\n",
      "         tips: shashi            914      91.86\n",
      "               nithin            864      86.83\n",
      "               shashi            857      86.13\n",
      "        tips: dheeraj            850      85.43\n",
      "         nithin_hours            847      85.13\n",
      "         shashi_hours            833      83.72\n",
      "                  jay            809      81.31\n",
      "              dheeraj            752      75.58\n",
      "                 yash            751      75.48\n",
      "           yash_hours            694      69.75\n",
      "        dheeraj_hours            653      65.63\n",
      "            file_year            437      43.92\n",
      "                 date             18       1.81\n",
      "                  day             17       1.71\n",
      "                total             14       1.41\n",
      "                 time             14       1.41\n"
     ]
    }
   ],
   "source": [
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total Rows: {len(combined_df)}\")\n",
    "print(f\"Total Columns: {len(combined_df.columns)}\")\n",
    "print(f\"\\nColumn Names:\")\n",
    "for i, col in enumerate(combined_df.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA TYPES\")\n",
    "print(\"=\"*60)\n",
    "print(combined_df.dtypes)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MISSING VALUES\")\n",
    "print(\"=\"*60)\n",
    "missing = combined_df.isnull().sum()\n",
    "missing_pct = (missing / len(combined_df) * 100).round(2)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing.index,\n",
    "    'Missing Count': missing.values,\n",
    "    'Missing %': missing_pct.values\n",
    "})\n",
    "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
    "print(missing_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "quality-report",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATA QUALITY REPORT BY FILE\n",
      "============================================================\n",
      "                file  rows  columns  missing_dates  missing_totals  duplicate_rows                                 date_range\n",
      "     April 2025.xlsx    32       24              2               1               1 (2025-04-01 00:00:00, 2025-04-30 00:00:00)\n",
      "        April24.xlsx    30       25              0               0               0 (2024-04-01 00:00:00, 2024-04-30 00:00:00)\n",
      "    August 2023.xlsx    31       22              0               0               0 (2023-08-01 00:00:00, 2023-08-31 00:00:00)\n",
      "    August 2025.xlsx    31       24              1               0               0 (2025-08-01 00:00:00, 2025-08-30 00:00:00)\n",
      "      August 24.xlsx    31       25              0               0               0 (2024-08-01 00:00:00, 2024-08-31 00:00:00)\n",
      "  December 2023.xlsx    31       24              0               1               0 (2023-12-01 00:00:00, 2023-12-31 00:00:00)\n",
      "  December 2025.xlsx    31       17              0               0               0 (2025-12-01 00:00:00, 2025-12-31 00:00:00)\n",
      "   December2022.xlsx    31       22              0               0               0 (2022-12-01 00:00:00, 2022-12-31 00:00:00)\n",
      "   December2024.xlsx    31       25              0               0               0 (2024-12-01 00:00:00, 2024-12-31 00:00:00)\n",
      "       Feb 2023.xlsx    31       22              0               0               0 (2023-02-01 00:00:00, 2023-03-03 00:00:00)\n",
      "        Feb2024.xlsx    32       23              2               3               1 (2024-01-01 00:00:00, 2024-01-29 00:00:00)\n",
      "        Feb2025.xlsx    31       25              3               0               2 (2025-02-01 00:00:00, 2025-02-28 00:00:00)\n",
      "       Jan 2024.xlsx    31       24              0               0               0 (2024-01-01 00:00:00, 2024-01-31 00:00:00)\n",
      "        Jan2025.xlsx    32       25              1               1               0 (2025-01-01 00:00:00, 2025-01-31 00:00:00)\n",
      "   January 2026.xlsx    31       17              0               0               0 (2026-01-01 00:00:00, 2026-01-31 00:00:00)\n",
      "      July 2023.xlsx    31       22              0               0               0 (2023-07-01 00:00:00, 2023-07-31 00:00:00)\n",
      "      July 2025.xlsx    31       24              0               0               0 (2025-07-01 00:00:00, 2025-07-31 00:00:00)\n",
      "        July 24.xlsx    32       25              1               1               0 (2024-07-01 00:00:00, 2024-07-31 00:00:00)\n",
      "      June 2025.xlsx    30       24              0               0               0 (2025-06-01 00:00:00, 2025-06-30 00:00:00)\n",
      "         June24.xlsx    32       25              2               2               1 (2024-06-01 00:00:00, 2024-06-30 00:00:00)\n",
      "     March 2024.xlsx    32       24              1               1               0 (2024-03-01 00:00:00, 2024-03-31 00:00:00)\n",
      "      March2025.xlsx    32       21              1               0               0 (2025-03-01 00:00:00, 2025-03-31 00:00:00)\n",
      "       May 2025.xlsx    31       24              0               0               0 (2025-05-01 00:00:00, 2025-05-31 00:00:00)\n",
      "  NOVEMBER 2022.xlsx    31       23              1               2               0 (2022-11-01 00:00:00, 2022-11-30 00:00:00)\n",
      "  November 2025.xlsx    30       24              0               0               0 (2025-11-01 00:00:00, 2025-11-30 00:00:00)\n",
      "    November 24.xlsx    31       25              1               0               0 (2024-11-01 00:00:00, 2024-11-30 00:00:00)\n",
      "   November2023.xlsx    31       23              0               1               0 (2023-11-01 00:00:00, 2023-12-01 00:00:00)\n",
      "   October 2023.xlsx    31       24              0               1               0 (2023-10-01 00:00:00, 2023-10-31 00:00:00)\n",
      "   October 2025.xlsx    31       24              0               0               0 (2025-10-01 00:00:00, 2025-10-31 00:00:00)\n",
      "     October 24.xlsx    31       25              0               0               0 (2024-10-01 00:00:00, 2024-10-31 00:00:00)\n",
      "    September24.xlsx    30       25              0               0               0 (2024-09-01 00:00:00, 2024-09-30 00:00:00)\n",
      "Septermber 2025.xlsx    31       24              1               0               0 (2025-09-01 00:00:00, 2025-09-30 00:00:00)\n",
      "\n",
      "============================================================\n",
      "SUMMARY STATISTICS\n",
      "============================================================\n",
      "Total rows across all files: 995\n",
      "Average rows per file: 31.1\n",
      "Files with missing dates: 12\n",
      "Files with missing totals: 10\n",
      "Files with duplicate rows: 4\n"
     ]
    }
   ],
   "source": [
    "# Display quality reports\n",
    "if quality_reports:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"DATA QUALITY REPORT BY FILE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    quality_df = pd.DataFrame(quality_reports)\n",
    "    print(quality_df.to_string(index=False))\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUMMARY STATISTICS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total rows across all files: {quality_df['rows'].sum()}\")\n",
    "    print(f\"Average rows per file: {quality_df['rows'].mean():.1f}\")\n",
    "    print(f\"Files with missing dates: {(quality_df['missing_dates'] > 0).sum()}\")\n",
    "    print(f\"Files with missing totals: {(quality_df['missing_totals'] > 0).sum()}\")\n",
    "    print(f\"Files with duplicate rows: {(quality_df['duplicate_rows'] > 0).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "error-report",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display error report\n",
    "if errors:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ERROR REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    for error in errors:\n",
    "        print(f\"\\nFile: {error['file']}\")\n",
    "        print(f\"Error: {error['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "save-outputs",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-28 09:21:33,247 - INFO - ðŸ’¾ Saved to CSV: ../datasets/combined_clean.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SAVING OUTPUTS\n",
      "============================================================\n",
      "ðŸ’¾ Saved to CSV: ../datasets/combined_clean.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-28 09:21:33,423 - INFO - ðŸ’¾ Saved to Excel: ../merged_data.xlsx\n",
      "2026-01-28 09:21:33,424 - INFO - âœ… Data aggregation complete!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¾ Saved to Excel: ../merged_data.xlsx\n",
      "\n",
      "âœ… Data aggregation complete!\n"
     ]
    }
   ],
   "source": [
    "# Save outputs\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING OUTPUTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save to CSV\n",
    "combined_df.to_csv(OUTPUT_CSV, index=False)\n",
    "logger.info(f\"ðŸ’¾ Saved to CSV: {OUTPUT_CSV}\")\n",
    "print(f\"ðŸ’¾ Saved to CSV: {OUTPUT_CSV}\")\n",
    "\n",
    "# Save to Excel\n",
    "combined_df.to_excel(OUTPUT_EXCEL, index=False)\n",
    "logger.info(f\"ðŸ’¾ Saved to Excel: {OUTPUT_EXCEL}\")\n",
    "print(f\"ðŸ’¾ Saved to Excel: {OUTPUT_EXCEL}\")\n",
    "\n",
    "print(\"\\nâœ… Data aggregation complete!\")\n",
    "logger.info(\"âœ… Data aggregation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "preview-data",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PREVIEW OF COMBINED DATA\n",
      "============================================================\n",
      "        date        day     time    jay  dheeraj  shashi  total  dheeraj_hours  shashi_hours  tips: shashi  tips: dheeraj      source_file  file_month  file_year  yash  nithin  yash_hours  total_employees_hours  total_employees  nithin_hours  tips: yash  sunny  sunny_hours  siddharth  atharv  atharv_hours\n",
      "0 2025-04-01    Tuesday  11 TO 8  306.0      NaN     NaN  306.0            NaN           NaN           NaN            NaN  April 2025.xlsx           4     2025.0   NaN     NaN         NaN                    NaN              NaN           NaN         NaN    NaN          NaN        NaN     NaN           NaN\n",
      "1 2025-04-02  Wednesday  11 TO 8    NaN    117.0     NaN  117.0            5.0           NaN           NaN            NaN  April 2025.xlsx           4     2025.0   NaN     NaN         NaN                    NaN              NaN           NaN         NaN    NaN          NaN        NaN     NaN           NaN\n",
      "2 2025-04-03   Thursday  11 TO 8  405.0      NaN     NaN  405.0            NaN           NaN           NaN            NaN  April 2025.xlsx           4     2025.0   NaN     NaN         NaN                    NaN              NaN           NaN         NaN    NaN          NaN        NaN     NaN           NaN\n",
      "3 2025-04-04     Friday  11 TO 9    NaN    650.0     NaN  650.0            5.5           9.0           2.5          13.64  April 2025.xlsx           4     2025.0   NaN     NaN         NaN                    NaN              NaN           NaN         NaN    NaN          NaN        NaN     NaN           NaN\n",
      "4 2025-04-05   Saturday  10 TO 9    NaN    615.0     NaN  615.0           10.0           6.0           NaN           3.00  April 2025.xlsx           4     2025.0   NaN     NaN         NaN                    NaN              NaN           NaN         NaN    NaN          NaN        NaN     NaN           NaN\n",
      "5 2025-04-06     Sunday  11 TO 7    NaN      NaN   370.0  370.0            NaN           8.0           4.0            NaN  April 2025.xlsx           4     2025.0   NaN     NaN         NaN                    NaN              NaN           NaN         NaN    NaN          NaN        NaN     NaN           NaN\n",
      "6 2025-04-07     Monday  11 TO 8    NaN      NaN   175.0  175.0            NaN           9.0           NaN            NaN  April 2025.xlsx           4     2025.0   NaN     NaN         NaN                    NaN              NaN           NaN         NaN    NaN          NaN        NaN     NaN           NaN\n",
      "7 2025-04-08    Tuesday  11 TO 8  235.0      NaN     NaN  235.0            NaN           NaN           NaN            NaN  April 2025.xlsx           4     2025.0   NaN     NaN         NaN                    NaN              NaN           NaN         NaN    NaN          NaN        NaN     NaN           NaN\n",
      "8 2025-04-09  Wednesday  11 TO 8  150.0      NaN     NaN  150.0            NaN           NaN           NaN            NaN  April 2025.xlsx           4     2025.0   NaN     NaN         NaN                    NaN              NaN           NaN         NaN    NaN          NaN        NaN     NaN           NaN\n",
      "9 2025-04-10   Thursday  11 TO 8    NaN    210.0     NaN  210.0            4.0           NaN           NaN            NaN  April 2025.xlsx           4     2025.0   NaN     NaN         NaN                    NaN              NaN           NaN         NaN    NaN          NaN        NaN     NaN           NaN\n",
      "\n",
      "============================================================\n",
      "DATA INFO\n",
      "============================================================\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 995 entries, 0 to 994\n",
      "Data columns (total 26 columns):\n",
      " #   Column                 Non-Null Count  Dtype         \n",
      "---  ------                 --------------  -----         \n",
      " 0   date                   977 non-null    datetime64[ns]\n",
      " 1   day                    978 non-null    object        \n",
      " 2   time                   981 non-null    object        \n",
      " 3   jay                    186 non-null    float64       \n",
      " 4   dheeraj                243 non-null    float64       \n",
      " 5   shashi                 138 non-null    float64       \n",
      " 6   total                  981 non-null    float64       \n",
      " 7   dheeraj_hours          342 non-null    float64       \n",
      " 8   shashi_hours           162 non-null    float64       \n",
      " 9   tips: shashi           81 non-null     float64       \n",
      " 10  tips: dheeraj          145 non-null    float64       \n",
      " 11  source_file            995 non-null    object        \n",
      " 12  file_month             995 non-null    int64         \n",
      " 13  file_year              558 non-null    float64       \n",
      " 14  yash                   244 non-null    float64       \n",
      " 15  nithin                 131 non-null    float64       \n",
      " 16  yash_hours             301 non-null    float64       \n",
      " 17  total_employees_hours  30 non-null     float64       \n",
      " 18  total_employees        30 non-null     float64       \n",
      " 19  nithin_hours           148 non-null    float64       \n",
      " 20  tips: yash             41 non-null     float64       \n",
      " 21  sunny                  3 non-null      float64       \n",
      " 22  sunny_hours            23 non-null     float64       \n",
      " 23  siddharth              0 non-null      float64       \n",
      " 24  atharv                 1 non-null      float64       \n",
      " 25  atharv_hours           3 non-null      float64       \n",
      "dtypes: datetime64[ns](1), float64(21), int64(1), object(3)\n",
      "memory usage: 202.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# Preview the final dataset\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREVIEW OF COMBINED DATA\")\n",
    "print(\"=\"*60)\n",
    "print(combined_df.head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA INFO\")\n",
    "print(\"=\"*60)\n",
    "combined_df.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
